# crawlers/kiat.py
from __future__ import annotations
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from urllib.parse import urljoin, quote_plus
import requests
from bs4 import BeautifulSoup
from pathlib import Path

BASE = "https://www.kiat.or.kr"
LIST_PAGE = f"{BASE}/front/board/boardContentsListPage.do"
LIST_AJAX = f"{BASE}/front/board/boardContentsListAjax.do"
VIEW_PAGE = f"{BASE}/front/board/boardContentsViewPage.do"

BOARD_ID = "90"  # ì‚¬ì—…ê³µê³ 
MENU_ID  = "b159c9dac684471b87256f1e25404f5e"

def _norm_date(s: Optional[str]) -> Optional[str]:
    if not s:
        return None
    s = s.strip().replace(".", "-").replace("/", "-")
    try:
        return datetime.strptime(s[:10], "%Y-%m-%d").strftime("%Y-%m-%d")
    except Exception:
        return (s[:10] or None)

def _parse_dt(s: Optional[str]) -> datetime:
    try:
        return datetime.strptime(s, "%Y-%m-%d")
    except Exception:
        return datetime.min

def _txt(el) -> str:
    return el.get_text(strip=True) if el else ""

def _norm_period(s: str) -> dict:
    raw = (s or "").strip()
    m = re.findall(r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})", raw)
    start = _norm_date(m[0]) if len(m) >= 1 else None
    end   = _norm_date(m[1]) if len(m) >= 2 else None
    pretty = f"{start} ~ {end}" if start or end else raw
    return {"start_date": start, "end_date": end, "period": pretty, "raw": raw}

def make_search_link(title: str) -> str:
    from urllib.parse import quote_plus
    kw = quote_plus(title)
    return (f"{LIST_PAGE}?board_id={BOARD_ID}&MenuId={MENU_ID}"
            f"&srchGubun=TITLE&srchKwd={kw}")


def fetch_kiat_notices(
    max_pages: int = 2,
    timeout: int = 20,
    debug_dir: Optional[Path] = None
) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    seen: set[str] = set()

    sess = requests.Session()
    common_headers = {
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/124.0 Safari/537.36"),
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept": "text/html, */*; q=0.01",
        "Connection": "keep-alive",
    }

    # ì´ˆê¸° ì§„ì…(ì¿ í‚¤/ì„¸ì…˜)
    sess.get(
        LIST_PAGE,
        params={"board_id": BOARD_ID, "MenuId": MENU_ID},
        headers={**common_headers, "Referer": BASE},
        timeout=timeout,
    )

    for page in range(1, max_pages + 1):
        try:
            data = {
                "board_id": BOARD_ID,
                "MenuId": MENU_ID,
                "pageIndex": str(page),
                "pageSize": "15",
                "srchGubun": "",
                "srchKwd": "",
            }
            headers = {
                **common_headers,
                "X-Requested-With": "XMLHttpRequest",
                "ajax": "true",
                "Origin": BASE,
                "Referer": f"{LIST_PAGE}?board_id={BOARD_ID}&MenuId={MENU_ID}",
                "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            }
            r = sess.post(LIST_AJAX, data=data, headers=headers, timeout=timeout)
            r.raise_for_status()
            html = r.text

            if debug_dir:
                Path(debug_dir).mkdir(parents=True, exist_ok=True)
                (Path(debug_dir) / f"kiat_p{page}.html").write_text(html, encoding="utf-8")

            soup = BeautifulSoup(html, "html.parser")
            rows = soup.select("table.list tbody tr")
            if not rows:
                continue

            for tr in rows:
                a = tr.select_one(".td_title a") or tr.find("a")
                if not a:
                    continue

                title = _txt(a)
                href = a.get("href", "")
                m = re.search(r"contentsView\('([^']+)'\)", href)
                contents_id = m.group(1) if m else None

                if contents_id and contents_id in seen:
                    continue
                if contents_id:
                    seen.add(contents_id)

                td_reg  = tr.find("td", class_=lambda c: c and ("td_reg_date" in c or "td_write_date" in c))
                td_term = tr.find("td", class_=lambda c: c and ("td_app_term" in c or "td_app_period" in c))

                reg_date = _norm_date(_txt(td_reg))
                period_info = _norm_period(_txt(td_term))

                # ğŸ”— ìƒì„¸ ë·° URL(ì§ì ‘ ì ‘ê·¼ ì‹œ ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ) + ê²€ìƒ‰ ë§í¬(ê¶Œì¥)
                view_link = (f"{VIEW_PAGE}?board_id={BOARD_ID}&MenuId={MENU_ID}&contents_id={contents_id}"
                             if contents_id else urljoin(BASE, href) if href and not href.startswith("javascript:")
                else f"{LIST_PAGE}?board_id={BOARD_ID}&MenuId={MENU_ID}")
                safe_link = make_search_link(title)

                # âœ… ì‚¬ìš©ì í´ë¦­ìš©: í”„ë¡ì‹œ ë§í¬ ìš°ì„ (ì›í´ë¦­ ìƒì„¸)
                proxy_link = f"/proxy/kiat/{contents_id}?t={quote_plus(title)}" if contents_id else safe_link


                items.append({
                    "source": "KIAT",
                    "title": title,
                    "link": proxy_link,  # â† ì—¬ê¸°!
                    "date": reg_date,
                    "institution": "ì‚°ì—…í†µìƒìì›ë¶€ > í•œêµ­ì‚°ì—…ê¸°ìˆ ì§„í¥ì›",
                    "meta": {
                        "contents_id": contents_id or "",
                        "ê³µê³ ëª…": title,
                        "ê³µê³ ì¼ì": reg_date or "",
                        "ì ‘ìˆ˜ê¸°ê°„": period_info["period"],
                        "ì ‘ìˆ˜ì‹œì‘": period_info["start_date"] or "",
                        "ì ‘ìˆ˜ì¢…ë£Œ": period_info["end_date"] or "",
                        "ì†Œê´€ë¶€ì²˜": "ì‚°ì—…í†µìƒìì›ë¶€",
                        "ì „ë¬¸ê¸°ê´€": "í•œêµ­ì‚°ì—…ê¸°ìˆ ì§„í¥ì›",
                        # ğŸ” ë°±ì—… ë§í¬: í˜¹ì‹œ í”„ë¡ì‹œê°€ ë§‰íˆë©´ UIì—ì„œ ë³´ì¡°ë¡œ ë…¸ì¶œ ê°€ëŠ¥
                        "backup_links": {
                            "proxy": proxy_link,
                            "view": view_link,
                            "list": f"{LIST_PAGE}?board_id={BOARD_ID}&MenuId={MENU_ID}",
                            "search": safe_link,
                        },
                    },
                })

        except Exception as e:
            if debug_dir:
                Path(debug_dir).mkdir(parents=True, exist_ok=True)
                (Path(debug_dir) / f"kiat_error_p{page}.txt").write_text(str(e), encoding="utf-8")
            continue

    items.sort(key=lambda it: _parse_dt(it.get("date")), reverse=True)
    return items


# ---------------------- ë©”ì¸ ì‹¤í–‰ë¶€ (ë‹¨ë… í…ŒìŠ¤íŠ¸ìš©) ----------------------
if __name__ == "__main__":
    import argparse, json

    parser = argparse.ArgumentParser(description="KIAT ì‚¬ì—…ê³µê³  í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--pages", type=int, default=1, help="ê°€ì ¸ì˜¬ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 1)")
    parser.add_argument("--limit", type=int, default=20, help="í‘œì‹œ ê°œìˆ˜ (ê¸°ë³¸ 20)")
    parser.add_argument("--debug-dir", type=str, default="", help="ì›ë¬¸ HTML ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--json", action="store_true", help="JSON í˜•íƒœë¡œ ì¶œë ¥")
    args = parser.parse_args()

    dbg = Path(args.debug_dir) if args.debug_dir else None
    data = fetch_kiat_notices(max_pages=args.pages, debug_dir=dbg)

    if args.json:
        print(json.dumps(data[: args.limit], ensure_ascii=False, indent=2))
    else:
        print(f"[KIAT] ìˆ˜ì§‘ {len(data)}ê±´ (í‘œì‹œ {min(len(data), args.limit)}ê±´) â€” ìµœì‹ ìˆœ")
        for row in data[: args.limit]:
            print(f"{row.get('date','')} | {row.get('title','')} | {row.get('link','')}")
