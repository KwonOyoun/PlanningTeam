# crawlers/g2b_api.py
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, re, time
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urlencode

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from zoneinfo import ZoneInfo  # Windowsë©´ 'tzdata' ì„¤ì¹˜ í•„ìš”: pip install tzdata

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ë³€ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
SERVICE_KEY = os.getenv("G2B_API_KEY")
KST = ZoneInfo("Asia/Seoul")

# ì œëª© í‚¤ì›Œë“œ(ê¸°ë³¸): ì—¬ê¸° ë“¤ì–´ìˆëŠ” ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ í†µê³¼
DEFAULT_TITLE_KEYWORDS = ["ì˜ë£Œê¸°ê¸°", "í—¬ìŠ¤ì¼€ì–´"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _now() -> datetime:
    # tz-aware KST ì‹œê°
    return datetime.now(KST)

def _txt(el) -> str:
    return el.get_text(strip=True) if el else ""

def _parse_dt_loose(s: str) -> Optional[datetime]:
    """ì—¬ëŸ¬ í¬ë§·ì„ ëŠìŠ¨í•˜ê²Œ íŒŒì‹±í•˜ê³  KSTë¡œ tz-aware ë°˜í™˜"""
    s = (s or "").strip()
    fmts = [
        "%Y-%m-%d %H:%M", "%Y.%m.%d %H:%M", "%Y/%m/%d %H:%M",
        "%Y-%m-%d", "%Y.%m.%d", "%Y/%m/%d",
    ]
    for f in fmts:
        try:
            dt = datetime.strptime(s[:16], f)
            return dt.replace(tzinfo=KST)
        except Exception:
            pass
    return None

def _ceil_div(a: int, b: int) -> int:
    return (a + b - 1) // b if b else 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# G2B API í´ë¼ì´ì–¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class G2BClient:
    service_key: str
    sess: requests.Session = requests.Session()

    BASE = "http://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
    EP_NOTICE = "getDataSetOpnStdBidPblancInfo"  # ê³µê³ ì •ë³´

    def __post_init__(self):
        self.sess.headers.update({
            "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                           "Chrome/124.0 Safari/537.36"),
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
        })

    def _get(self, endpoint: str, params: Dict[str, str]) -> str:
        url = f"{self.BASE}/{endpoint}?{urlencode({**params, 'ServiceKey': self.service_key})}"
        r = self.sess.get(url, timeout=25)
        r.raise_for_status()
        return r.text

    # A) ê²Œì‹œ(ê³µê³ )ì¼ì‹œ êµ¬ê°„ ì¡°íšŒ
    def get_by_posted(self, start_dt: datetime, end_dt: datetime, *, rows: int, page: int) -> str:
        if (end_dt - start_dt).days > 31:
            start_dt = end_dt - timedelta(days=31)
        params = {
            "bidNtceBgnDt": start_dt.strftime("%Y%m%d%H%M"),
            "bidNtceEndDt": end_dt.strftime("%Y%m%d%H%M"),
            "numOfRows": str(rows),
            "pageNo": str(page),
            # íŒíŠ¸(ë¬´ì‹œë  ìˆ˜ ìˆìŒ)
            "bsnsDivCd": "5",  # ìš©ì—­
        }
        return self._get(self.EP_NOTICE, params)

    # B) (êµ¬) ì…ì°°ì‹œì‘~ë§ˆê°ì¼ì‹œ êµ¬ê°„ ì¡°íšŒ
    def get_by_deadline(self, start_dt: datetime, end_dt: datetime, *, rows: int, page: int) -> str:
        params = {
            "bidBeginDate": start_dt.strftime("%Y%m%d%H%M"),
            "bidClseDate":  end_dt.strftime("%Y%m%d%H%M"),
            "numOfRows": str(rows),
            "pageNo": str(page),
        }
        return self._get(self.EP_NOTICE, params)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒì„¸í˜ì´ì§€ì—ì„œ 'ê²Œì‹œì¼ì‹œ' ê¸ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DETAIL_URL_TMPL = ("https://www.g2b.go.kr/ep/invitation/publish/"
                   "bidInfoDtl.do?bidno={no}&bidseq={seq}&releaseYn=Y")

_LABEL_PAT = re.compile("(ê²Œì‹œì¼ì‹œ|ê³µê³ ê²Œì‹œì¼ì‹œ|ê³µê³ ì¼ì‹œ|ì…ì°°ê³µê³ ì¼ì‹œ|ë“±ë¡ì¼|ë“±ë¡ì¼ì‹œ)")

def _detail_url_from_item(m: Dict[str, str]) -> Optional[str]:
    url = (m.get("bidNtceUrl") or "").strip()
    if url:
        return url
    no  = (m.get("bidNtceNo")  or "").strip()
    seq = (m.get("bidNtceOrd") or "").strip() or "00"
    if no:
        return DETAIL_URL_TMPL.format(no=no, seq=seq)
    return None

def _scrape_posted_dt(url: str, sess: requests.Session, timeout: int = 20) -> Optional[datetime]:
    """ìƒì„¸ í˜ì´ì§€ HTMLì—ì„œ ê²Œì‹œì¼/ê³µê³ ì¼ì‹œë¥¼ ìµœëŒ€í•œ ì°¾ì•„ KST tz-awareë¡œ ë°˜í™˜"""
    try:
        r = sess.get(url, timeout=timeout)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        # 1) ë¼ë²¨ ì˜† ê°’
        for lab in soup.find_all(text=_LABEL_PAT):
            node = getattr(lab, "parent", None)
            if not node:
                continue
            candidates = [
                node.find_next_sibling(),
                node.parent.find_next_sibling() if getattr(node, "parent", None) else None,
            ]
            for cand in candidates:
                dt = _parse_dt_loose(_txt(cand))
                if dt:
                    return dt

        # 2) ë¬¸ì„œ ì „ì²´ì—ì„œ ì •ê·œì‹
        text = soup.get_text(" ", strip=True)
        m = re.search(r"(ê²Œì‹œì¼ì‹œ|ê³µê³ ê²Œì‹œì¼ì‹œ|ê³µê³ ì¼ì‹œ|ì…ì°°ê³µê³ ì¼ì‹œ|ë“±ë¡ì¼|ë“±ë¡ì¼ì‹œ)\s*[:ï¼š]?\s*"
                      r"([0-9]{4}[-./][0-9]{2}[-./][0-9]{2})(\s*[0-9]{2}:[0-9]{2})?", text)
        if m:
            raw = (m.group(2) + (m.group(3) or "")).strip()
            return _parse_dt_loose(raw)
    except Exception:
        pass
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ìˆ˜ì§‘ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_g2b_service_notices(
    *,
    days_back: int = 5,       # ìµœê·¼ Nì¼
    rows: int = 100,          # í˜ì´ì§€ë‹¹ í–‰ìˆ˜
    scan_pages: int = 80,     # ì—­í˜ì´ì§•ìœ¼ë¡œ ìµœëŒ€ ëª‡ í˜ì´ì§€ ìŠ¤ìº”í• ì§€(ëª¨ë“œë³„)
    prefer: str = "mix",      # "ntce" | "clse" | "mix"
    max_details: int = 300,   # ìƒì„¸ í˜ì´ì§€ ì¡°íšŒ ìƒí•œ(ê²Œì‹œì¼ì‹œ ë³´ì •ìš©)
    pause_sec: float = 0.1,   # API/ìƒì„¸ í˜¸ì¶œ ì‚¬ì´ ëŒ€ê¸°
    verbose: bool = False,
    keywords: Optional[List[str]] = None,  # â† ì œëª© í•„í„°(ê¸°ë³¸: ì˜ë£Œê¸°ê¸°/í—¬ìŠ¤ì¼€ì–´)
) -> List[Dict[str, Any]]:
    """
    â€¢ í•œêµ­ì‹œê°„(KST) ê¸°ì¤€
    â€¢ ìµœê·¼ Nì¼ 'ê²Œì‹œ'ëœ 'ìš©ì—­' ê³µê³ ë§Œ ëŒ€ìƒ
    â€¢ ë§ˆê°ì¼ì‹œê°€ í˜„ì¬ ì‹œê° ì´ì „(< now)ì€ ì œì™¸ (ì˜¤ëŠ˜ ë§ˆê°ì€ í¬í•¨)
    â€¢ ntce(ê²Œì‹œì¼ì‹œ) â†’ clse(ë§ˆê°êµ¬ê°„) ìˆœìœ¼ë¡œ ì—­í˜ì´ì§• ìŠ¤ìº”í•˜ì—¬ ìµœì‹ ë¶€í„° í™•ë³´
    â€¢ ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ê²Œì‹œì¼ì‹œ'ë¥¼ ìµœëŒ€í•œ ë³´ì •
    â€¢ ì œëª©ì— keywords ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ê²½ìš°ë§Œ ìˆ˜ì§‘
    """
    if not SERVICE_KEY:
        raise RuntimeError("G2B_API_KEY(.env)ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    kw_list = [k for k in (keywords if keywords is not None else DEFAULT_TITLE_KEYWORDS) if k]

    api  = G2BClient(SERVICE_KEY)
    now  = _now()
    # ì˜¤ëŠ˜ 00:00ë¶€í„° days_backì¼ ë²”ìœ„
    start = now.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=days_back-1)
    end   = now

    mode_order = {"ntce": ["ntce"], "clse": ["clse"], "mix": ["ntce", "clse"]}[prefer]

    def _parse_items(xml: str) -> Tuple[List[Dict[str, str]], int]:
        root = ET.fromstring(xml)
        code = (root.findtext(".//header/resultCode") or "").strip()
        msg  = (root.findtext(".//header/resultMsg")  or "").strip()
        if code and code != "00":
            if verbose:
                print(f"[G2B] API ì˜¤ë¥˜ code={code} msg={msg}")
            return [], 0
        items = root.findall(".//body/items/item") or root.findall(".//item")
        total = int((root.findtext(".//body/totalCount") or "0").strip() or 0)
        rows_ = [{c.tag: (c.text or "").strip() for c in it} for it in items]
        return rows_, total

    def _collect(mode: str) -> List[Dict[str, Any]]:
        cand: List[Dict[str, Any]] = []
        seen_no_seq: set[Tuple[str, str]] = set()

        # 1) totalCount íŒŒì•…ìš© 1í˜ì´ì§€
        xml1 = api.get_by_posted(start, end, rows=rows, page=1) if mode == "ntce" \
            else api.get_by_deadline(start, end, rows=rows, page=1)
        items1, total = _parse_items(xml1)
        total_pages = max(1, _ceil_div(total, rows))
        if verbose:
            from collections import Counter
            dist = Counter((x.get("bsnsDivNm") or "").strip() for x in items1)
            print(f"[G2B] {mode} p1 rows={len(items1)} total={total} dist={dist.most_common(5)}")

        # 2) ìµœì‹ ë¶€í„° ì—­í˜ì´ì§•
        last = total_pages
        first = max(1, last - scan_pages + 1)
        page_range = range(last, first - 1, -1)

        for page in page_range:
            try:
                xml = api.get_by_posted(start, end, rows=rows, page=page) if mode == "ntce" \
                    else api.get_by_deadline(start, end, rows=rows, page=page)
                items, _ = _parse_items(xml)
                if not items:
                    continue

                for it in items:
                    # ì—…ë¬´êµ¬ë¶„: 'ìš©ì—­' í¬í•¨(ì¼ë°˜/í•™ìˆ /ê¸°íƒ€ìš©ì—­ ì»¤ë²„)
                    bsns = (it.get("bsnsDivNm") or "").strip()
                    if "ìš©ì—­" not in bsns:
                        continue

                    title = (it.get("bidNtceNm") or "").strip()
                    # ğŸ” ì œëª© í‚¤ì›Œë“œ í•„í„°(í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ í†µê³¼)
                    if kw_list and not any(kw in title for kw in kw_list):
                        continue

                    # ë§ˆê°ì¼ì‹œ(ì˜¤ëŠ˜ ë§ˆê° í¬í•¨, ì´ë¯¸ ì§€ë‚œ ê²ƒë§Œ ì œì™¸)
                    d, t = (it.get("bidClseDate") or "").strip(), (it.get("bidClseTm") or "").strip()
                    if not d or not t:
                        continue
                    try:
                        clse_dt = datetime.strptime(f"{d} {t}", "%Y-%m-%d %H:%M").replace(tzinfo=KST)
                        if clse_dt < now:
                            continue
                    except Exception:
                        continue

                    # ì¤‘ë³µ ì œê±°
                    no  = (it.get("bidNtceNo")  or "").strip()
                    seq = (it.get("bidNtceOrd") or "").strip()
                    key = (no, seq)
                    if no and key in seen_no_seq:
                        continue
                    if no:
                        seen_no_seq.add(key)

                    # APIì—ì„œ 'ê²Œì‹œì¼ì‹œ'ê°€ ì˜¤ë©´ ìš°ì„  ì‚¬ìš©
                    api_posted = None
                    nd, nt = (it.get("bidNtceDate") or "").strip(), (it.get("bidNtceBgn") or "").strip()
                    if nd:
                        api_posted = _parse_dt_loose((nd + (" " + nt if nt else "")).strip())

                    cand.append({
                        "title": title,
                        "institution": (it.get("ntceInsttNm") or "").strip(),
                        "link": _detail_url_from_item(it) or "",
                        "meta": it,
                        "_clse_dt": clse_dt,
                        "_posted": api_posted,
                        "_need_detail": (mode == "clse") or (api_posted is None),
                    })

                time.sleep(pause_sec)
            except Exception as e:
                if verbose:
                    print(f"[G2B] {mode} page {page} error:", e)
                time.sleep(pause_sec)
                continue

        return cand

    final: List[Dict[str, Any]] = []
    for mode in mode_order:
        cand = _collect(mode)

        # ìƒì„¸ í˜ì´ì§€ë¡œ 'ê²Œì‹œì¼ì‹œ' ë³´ì •
        fixed = 0
        for c in cand:
            if c["_need_detail"] and c["link"] and fixed < max_details and c.get("_posted") is None:
                dt = _scrape_posted_dt(c["link"], api.sess, timeout=20)
                if dt:
                    c["_posted"] = dt
                fixed += 1

        # ìµœê·¼ Nì¼(ê²Œì‹œì¼ì‹œ) + ë§ˆê° ë¯¸ê²½ê³¼ í•„í„°ë§
        for c in cand:
            p = c["_posted"]
            if not p:
                continue
            if not (start <= p <= end):
                continue
            if c["_clse_dt"] < now:
                continue

            final.append({
                "title": c["title"],
                "date": p.strftime("%Y-%m-%d"),                 # í™”ë©´ ì •ë ¬ìš©(ì¼ ë‹¨ìœ„)
                "end_date": c["_clse_dt"].strftime("%Y-%m-%d %H:%M"),
                "link": c["link"],
                "institution": c["institution"],
                "notice_posted_at": p.strftime("%Y-%m-%d %H:%M"),  # ê²Œì‹œì¼ì‹œ(í‘œì‹œ)
                "meta": c["meta"],
            })

        if final:
            break  # í˜„ì¬ ëª¨ë“œì—ì„œ ì¶©ë¶„íˆ í™•ë³´ë˜ë©´ ë‹¤ìŒ ëª¨ë“œë¡œ ì•ˆ ë„˜ì–´ê°

    # ì •ë ¬: ê²Œì‹œì¼ì‹œ(ë¬¸ì) â†’ datetime íŒŒì‹± í›„ ë‚´ë¦¼ì°¨ìˆœ
    def _key(x):
        return _parse_dt_loose(x.get("notice_posted_at") or x.get("date") or "") or datetime.min.replace(tzinfo=KST)

    final.sort(key=_key, reverse=True)
    return final

# --- backward-compat wrapper for main.py -------------------------------------
def fetch_g2b_notices(max_pages: int = 5,
                      rows: int = 50,
                      days: int = 5,
                      query: Optional[object] = None,
                      prefer: str = "mix"):
    """
    main.py í˜¸í™˜ìš© ë˜í¼.
    - queryê°€ ë¬¸ìì—´ì´ë©´ ê³µë°±/ì‰¼í‘œë¡œ ë¶„í• í•˜ì—¬ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
    - queryê°€ ë¦¬ìŠ¤íŠ¸/íŠœí”Œ/ì„¸íŠ¸ë©´ ê·¸ëŒ€ë¡œ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
    - ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ(DEFAULT_TITLE_KEYWORDS) ì ìš©
    """
    # ìµœì‹  í˜ì´ì§€ë¶€í„° ë” ê¹Šê²Œ ìŠ¤ìº”
    scan_pages = max(20, int(max_pages) * 20)

    # query â†’ keywords ë³€í™˜
    keywords: Optional[List[str]] = None
    if isinstance(query, str) and query.strip():
        import re as _re
        keywords = [s for s in _re.split(r"[,\s]+", query.strip()) if s]
    elif isinstance(query, (list, tuple, set)):
        keywords = [str(s) for s in query if str(s).strip()]
    else:
        keywords = None  # ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©

    items = fetch_g2b_service_notices(
        days_back=days,
        rows=rows,
        scan_pages=scan_pages,
        prefer=prefer,
        max_details=300,
        pause_sec=0.1,
        verbose=False,
        keywords=keywords,  # â† ì œëª© í•„í„° ë°˜ì˜
    )
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI í…ŒìŠ¤íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    items = fetch_g2b_service_notices(
        days_back=5,          # ìµœê·¼ 5ì¼
        rows=100,             # í˜ì´ì§€ë‹¹ 100í–‰
        scan_pages=80,        # ìµœì‹  ìª½ë¶€í„° 80í˜ì´ì§€ ìŠ¤ìº”
        prefer="mix",         # ntce ë¨¼ì €, ë¶€ì¡±í•˜ë©´ clse ë³´ê°•
        max_details=300,
        pause_sec=0.1,
        verbose=True,         # ë””ë²„ê·¸ ë¡œê·¸ ë³´ê³  ì‹¶ìœ¼ë©´ True
        keywords=None,        # Noneì´ë©´ ê¸°ë³¸ ["ì˜ë£Œê¸°ê¸°","í—¬ìŠ¤ì¼€ì–´"] ì ìš©
    )
    print(f"[G2B] ìµœê·¼ {5}ì¼(ì œëª© í‚¤ì›Œë“œ í•„í„°) : {len(items)}ê±´")
    for it in items[:80]:
        print(f"{it['notice_posted_at']} | {it['title']} | {it['institution']} | ë§ˆê° {it['end_date']}")
